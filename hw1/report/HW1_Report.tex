%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Stylish Curriculum Vitae
% LaTeX Template
% Version 1.1 (September 10, 2021)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% Authors:
% Stefano (https://www.kindoblue.nl)
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 4.0 (https://creativecommons.org/licenses/by-nc-sa/4.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% !TEX program = xelatex
\documentclass[a4paper, oneside, final, 12pt]{scrartcl} % Paper options using the scrartcl class

\usepackage{fontspec} % for other font
\usepackage{xeCJK} % for chinese font
\usepackage{hyperref} % for hyper web link
\usepackage{multirow} % for tabular table in learning progress
\usepackage{graphicx} % for image insersion
\usepackage[export]{adjustbox} % for image frame
\usepackage{setspace}
\usepackage{array}
% Define typographic struts, as suggested by Claudio Beccari
%   in an article in TeX and TUG News, Vol. 2, 1993.
\usepackage{mathptmx}
\usepackage{scrlayer-scrpage} % Provides headers and footers configuration
\usepackage{titlesec} % Allows creating custom \section's
\usepackage{marvosym} % Allows the use of symbols
\usepackage{tabularx,colortbl} % Advanced table configurations
% \usepackage{ebgaramond} % Use the EB Garamond font
\usepackage{microtype} % To enable letterspacing
\usepackage{pdfpages} % for showing pdf
\usepackage{pdflscape}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{listings}   % highlight the python code
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{cite} %Imports biblatex package
\usepackage[ruled,linesnumbered]{algorithm2e}
\newcommand\mycommfont[1]{\normalsize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
% \usepackage[backend=bibtex,bibencoding=ascii,style=authoryear,sorting=none]{bibtex}
% \addbibresource{reference.bib}
% setup the margin
\usepackage[top=1cm, bottom=1cm, right=2cm, left=2cm]{geometry}

% set the style of listing code
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=true,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% set chinese and english font
\setmainfont{Times New Roman}
\setCJKmainfont[AutoFakeBold=true, AutoFakeSlant=true]{標楷體}

\titleformat{\section}{\Large\raggedright\bfseries}{}{0em}{}[\titlerule] % Section formatting
\titleformat{\subsection}{\large\raggedright\bfseries}{}{0em}{}
\titleformat{\subsubsection}{\normalsize\raggedright\bfseries}{}{0em}{}

% \pagestyle{scrheadings} % Print the headers and footers on all pages

% enable bold and slant chinese font
% \xeCJKsetup{AutoFakeBold=true, AutoFakeSlant=true}

% set the space at the front of paragraph
\setlength{\parindent}{2em}

% disable page number
\pagenumbering{gobble}

\newcommand{\gray}{\rowcolor[gray]{.90}} % Custom highlighting for the work experience and education sections
\newcommand{\Tstrut}{\rule{0pt}{2.6ex}}         % = `top' strut
\newcommand{\Bstrut}{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut
\newcommand{\Tstruth}{\rule{0pt}{4ex}}         % = `top' strut for header
\newcommand{\Bstruth}{\rule[-2.5ex]{0pt}{0pt}}   % = `bottom' strut for header

%----------------------------------------------------------------------------------------
%	FOOTER SECTION
%----------------------------------------------------------------------------------------

% \renewcommand{\headfont}{\normalfont\rmfamily\scshape} % Font settings for footer

% \cofoot{
% \fontsize{12.5}{17}\selectfont % Letter spacing and font size

% \textls[150]{123 Broadway {\large\textperiodcentered} City {\large\textperiodcentered} Country 12345}\\ % Your mailing address
% {\Large\Letter} \textls[150]{john@smith.com \ {\Large\Telefon} (000) 111-1111} % Your email address and phone number
% }

%----------------------------------------------------------------------------------------
\begin{document}

%----------------------------------------------------------------------------------------
%	HEADER SECTION
%----------------------------------------------------------------------------------------


\begin{center}
    {\fontsize{18}{30}\textbf{Data Mining Assignment 1 \\ Association Rule Mining}}
\end{center}

\begin{center}
  Bo-Han Chen (陳柏翰) \\
  Student ID:312551074 \\
  bhchen312551074.cs12@nycu.edu.tw
\end{center}

\section{Experiment Environment \& Usage}

\begingroup
\raggedright

\subsection{Environment}

The Experiment environment is based on the work station of Information Technology Center,
the details are as follows:
\begin{itemize}
  \item OS: CentOS Stream release 8
  \item Hardware: Intel(R) Xeon(R) Gold 6126 CPU @ 2.60GHz
  \item Python 3.9.17
  \item The computation time is recorded by \emph{time.process\_time()} function
\end{itemize}

\subsection{Usage}

The command for executing the program of step2 \& 3 is shown as follows:

\begin{lstlisting}[language=bash]
  # step2
  python apriori.py -f [inputFile] -t [task] -s [support]
  # step3
  python myEclat.py -f [inputFile] -s [support]
\end{lstlisting}

I wrote a script for running the association rule mining program,
whcich can run the algorithm with all task/support/dataset options,
and the execution time will be recorded in the log file named \emph{result.log}.
Take the script of step2 for example, the script 
\emph{run.sh} is shown as follows:

\begin{lstlisting}[language=bash]
  #!/bin/bash
  dataset_folder="../dataset"
  log_file_path="../result"
  declare -a dataset_arr=("datasetA.data" "datasetB.data" "datasetC.data")
  declare -a task_arr=(1 2)
  declare -a sup_arrA=(0.2 0.5 0.1)
  declare -a sup_arrB=(0.5 0.2 0.5)
  declare -a sup_arrC=(0.1 0.2 0.3)

  for task in "${task_arr[@]}"
  do
      for sup_idx in 0 1 2
      do
          for dataset in "${dataset_arr[@]}"
          do
              if [ $dataset == 'datasetA.data' ]
              then
                  sup_arr=("${sup_arrA[@]}")
              elif [ $dataset == 'datasetB.data' ]
              then
                  sup_arr=("${sup_arrB[@]}")
              else [ $dataset == 'datasetC.data' ]
                  sup_arr=("${sup_arrC[@]}")
              fi
              data_path=$dataset_folder/$dataset
              sup="${sup_arr[$sup_idx]}"
              echo "running $dataset on task $task with support: $sup"
              time python apriori.py -f $data_path -t $task -s $sup | tee -a $log_file_path/result.log
          done
      done
  done
\end{lstlisting}

The usage of \emph{run.sh}:

\begin{lstlisting}[language=bash]
  # executing run.sh script
  ./run.sh
\end{lstlisting}

\endgroup

\section{Step2: Apriori Algorithm}

\subsection{Task1: Mining Frequent Itemsets}

\begingroup
\raggedright
In this part, I add two functions \emph{writeTask1\_1} and \emph{writeTask1\_2} to
write the frequent itemsets to the txt file based on the original Apriori algorithm. 
The code is shown as follows:

\begin{lstlisting}[language=Python]
  def runApriori_1(data_iter, case, minSupport):
    itemSet, transactionList = getItemSetTransactionList(data_iter)

    freqSet = defaultdict(int)
    largeSet = dict()
    # initialize the number of candidate itemset before and after pruning
    canNumSetBf = [len(itemSet)]
    canNumSetAf = []

    oneCSet= returnItemsWithMinSupport(itemSet, transactionList, minSupport, freqSet)
    canNumSetAf.append(len(oneCSet))
    
    currentLSet = oneCSet
    k = 2
    while currentLSet != set([]):    
        largeSet[k - 1] = currentLSet
        currentLSet = joinSet(currentLSet, k)
        # get the number of candidate itemset before pruning
        canNumSetBf.append(len(currentLSet))
        currentCSet= returnItemsWithMinSupport(
            currentLSet, transactionList, minSupport, freqSet
        )
        # get the number of candidate itemset after pruning
        canNumSetAf.append(len(currentCSet))
        currentLSet = currentCSet
        k = k + 1
    .
    .
    .
    # write the frequent itemsets and number of candidate to file
    writeTask1_1(toRetItems, case, minSupport)
    writeTask1_2(canNumSetBf, canNumSetAf, case, minSupport)
\end{lstlisting}

In \emph{writeTask1\_1} function, the frequent itemsets will be 
sorted by support and be written to the file.

\begin{lstlisting}[language=Python]
  def writeTask1_1(items, case, sup):
    """write the generated itemsets sorted by support to file"""
    write_line = ''
    for itemset, support in sorted(items, key=lambda x: x[1], reverse = True):
        item_str = ""
        for item in itemset:
            item_str = item_str + str(item) + ','
        item_str = item_str.strip(',')
        write_line += "%.1f\t{%s}\n" %(support * 100, item_str)
    with open('../result/' + 'step2' + '_task1_' + case + '_' + str(sup) + '_result1.txt', mode = 'w') as write_file:
        write_file.write(write_line)
\end{lstlisting}

In \emph{writeTask1\_2} function, the number of candidate itemsets before and after pruning
will be written to the file.

\begin{lstlisting}[language=Python]
  def writeTask1_2(canNumSetBf, canNumSetAf, case, sup):
    """write the number of candidate itemsets before and after pruning to file"""
    write_line = str(sum(canNumSetAf)) + '\n'
    for idx in range(len(canNumSetBf)):
        write_line += "%s\t%s\t%s\n" %(str(idx + 1), str(canNumSetBf[idx]), str(canNumSetAf[idx]))
    with open('../result/' + 'step2' + '_task1_' + case + '_' + str(sup) + '_result2.txt', mode = 'w') as write_file:
        write_file.write(write_line)
\end{lstlisting}

The computation time of task1 is shown as follows 
(concluded from the \emph{result.log} file):

\begin{table}[ht]
  \centering
    \begin{tabular}{|*{3}{c|}}
        \hline
    Dataset    & Minimum Support (\%)  & Computation Time (sec)  \\
        \hline
    \multirow[t]{3}{*}{A}           
                & \multirow[t]{3}{*}{}
                0.2            & 143.79 \\  \cline{2-3}
                & 0.5          & 6.72 \\  \cline{2-3}
                & 1.0          & 2.79 \\  \cline{1-3}         
                B & \multirow[t]{3}{*}{}
                0.15            & 6861.15 \\  \cline{2-3}
                & 0.2          & 3823.43 \\  \cline{2-3}
                & 0.5          & 1111.96 \\  \cline{1-3}
                C & \multirow[t]{3}{*}{}
                1.0            & 6074.08 \\  \cline{2-3}
                & 2.0          & 1994.86 \\  \cline{2-3}
                & 3.0          & 729.53 \\ 
        \hline
    \end{tabular}
  \caption{Computation Time of Task1}
\end{table}

As we can see above, the computation time increases considerablely
when the minimum support ($min\_sup$) decreases. Take dataset A for example,
and the computation time of $min\_sup = 0.5\%$ is $95\%$ faster than $min\_sup = 0.2\%$,
and the computation time of $min\_sup = 1.0\%$ is $98\%$ faster than $min\_sup = 0.2\%$. \\
To explain this phenomenon, we can analyze \emph{result2.txt} file to find out the reason.
Comparing the number of candidate k-itemsets ($L_k$) of each iteration 
among $min\_sup = 0.5\%$ and $min\_sup = 0.2\%$,
we can observe that with higher minimum support, 
fewer frequent k-itemsets ($F_k$) will remain in each iteration,
which leads to fewer procedure to calculate the support of itemsets in $L_{k+1}$.

\endgroup

\subsection{Task2: Mining All Frequent Closed Itemsets}

\begingroup
\raggedright

In this task, I first check whether the frequent itemset is closed or not by \emph{checkClosed}
function in each iteration, 
and then write the closed frequent itemsets to the file by \emph{writeTask2}.

\begin{lstlisting}[language=Python]
  def runApriori_2(data_iter, case, minSupport):
    itemSet, transactionList = getItemSetTransactionList(data_iter)
    ...
    k = 2
    # save the closed frequent itemsets in each iteration
    closedSet = dict()
    while currentLSet != set([]):    
        largeSet[k - 1] = currentLSet
        currentLSet = joinSet(currentLSet, k)
        currentCSet= returnItemsWithMinSupport(
            currentLSet, transactionList, minSupport, freqSet
        )
        # check whether the frequent itemset is closed or not
        # passing the frequent itemset in last iteration and current iteration
        closedSet[k - 1] = checkClosed(largeSet[k-1], currentCSet, freqSet)
        currentLSet = currentCSet
        k = k + 1
    ...
    # write the closed frequent itemsets to file
    closedItems = []
    for key, value in closedSet.items():
        closedItems.extend([(tuple(item), getSupport(item)) for item in value])

    writeTask2(closedItems, case, minSupport)
\end{lstlisting}

In \emph{checkClosed} function, each itemset of $F_{k-1}$ will be compared with
each itemset of $F_{k}$, if the latter one is a superset of the former and
the support of the latter is larger or equal (equal, precisely) to the former one, 
then we can say that itemset is not closed.

\begin{lstlisting}[language=Python]
  def checkClosed(canLevelPre, canLevelCur, freqSet):
    # first assume that all itemsets of previous iteration are closed
    closedSetPre = canLevelPre.copy()
    for item_pre in canLevelPre:
        for item_cur in canLevelCur:
            # if item_cur is a superset of item_pre
            # and the support of item_cur is larger than item_pre
            # then the latter one is not closed
            if item_pre.issubset(item_cur) and freqSet[item_pre] <= freqSet[item_cur]:
                closedSetPre.remove(item_pre)
                break
    return closedSetPre
\end{lstlisting}

The computation time of task2 and the comparison with task1 is shown as follows:

\begin{table}[ht]
  \centering
    \begin{tabular}{|*{4}{c|}}
        \hline
    Dataset & Minimum Support (\%)  & Computation Time (sec) & Ratio of Computation Time (\%)  \\
        \hline
    \multirow[t]{3}{*}{A}           
                & \multirow[t]{3}{*}{}
                0.2            & 157.117 & 109.26\% \\  \cline{2-4}
                & 0.5          & 6.65 & 98.95\% \\  \cline{2-4}
                & 1.0          & 2.70 & 96.77\% \\  \cline{1-4}         
                B & \multirow[t]{3}{*}{}
                0.15            & 7094.64 & 103.40\% \\  \cline{2-4}
                & 0.2          & 3730.72 & 97.57\% \\  \cline{2-4}
                & 0.5          & 1137.05 & 102.25\% \\  \cline{1-4}
                C & \multirow[t]{3}{*}{}
                1.0            & 6007.42 & 98.90\% \\  \cline{2-4}
                & 2.0          & 1962.21 & 98.36\% \\  \cline{2-4}
                & 3.0          & 717.23 & 98.31\% \\ 
        \hline
    \end{tabular}
  \caption{Computation Time of Task2}
\end{table}

With low $min\_sup$ (take datasetA with $min\_sup = 0.2\%$ for example), 
we can observe that task2 is obviously
slower than task1, since there are more itemsets in $F_{k-1}$ and $F_{k}$,
and there will also have more iteration in the while loop,
which cause more check procedure in \emph{checkClosed} function.
Sometimes the computation of task2 is even faser than task1,
by observing the \emph{result.log} file, we can find out
such condition is caused by the number of iteration, in other words,
if there is fewer iteration, the extra computation of \emph{checkClosed}
is nearly negligible.

\endgroup

\section{Step3: Eclat Algorithm}

\begingroup
\raggedright

For task3, I choose Eclat mining algorithm to mine the frequent itemsets.
In this section, I will first introduce the Eclat algorithm,
then explain its advantages compared to Apriori algorithm,
finally analysis the experiment result.

\subsection{Introduction}

Eclat algorithm\cite{zaki1997new}\cite{borgelt2012frequent} is a depth-first-based association mining algorithm 
using the vertical database, instead of calculating the support of each itemset by
traversing the whole trasaction list, Eclat algorithm uses the intersection of $TID\_Sets$,
which results in more efficient computation. \\
% The concept of my Eclat algorithm is based on \cite{github}, 
% which implement the Eclat algorithm with binary encoding database.

\subsection{Program Flow}

The pueudocode of Eclat algorithm is shown as follows:

\begin{algorithm}
  \caption{My Eclat Algorithm Overview}
  % \SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
  % \SetKwFunction{Union}{Union}
  \SetKwFunction{EclatRecursive}{EclatRecursive}
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

  \Input{Transaction list $T$, minimum support $Sup_{min}$}
  \Output{Frequent itemsets $F$}
  \BlankLine
  \tcp{build the vertical database $VDB$ from $T$}
  \For{$tran$ in $T$}{
    \For{$item$ in $tran$}{
      append $tran$ to $VDB[item]$
    }
  }
  \tcp{get the frequent 1-itemsets $F_1$ from $VDB$}
  \For{$item$ in $VDB$}{
    \If{$|VDB[item]| \geq Sup_{min}$}{
      append $item$ to $F_1$
    }
  }
  \tcp{mine the frequent itemsets recursively}
  $F = []$ \\
  \For{$item$ in $F_1$}{
    \EclatRecursive{$item$, $VDB[item]$, $idx(item)$}
  }
\end{algorithm}

First the vertical database $VDB$ will be built from the trasaction list $T$,
then the frequent 1-itemsets $F_1$ will be generated from $VDB$.
Finally, the frequent itemsets will be mined recursively by \emph{EclatRecursive} function.
Why we need to build $F_1$ before calling \emph{EclatRecursive} function will be discussed later.\\

The following algorithm represents the \emph{EclatRecursive} function:

\begin{algorithm}
  \caption{EclatRecursive Function}
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

  \Input{frequent itemset $i$, tid set $SET_{i}$, index of $i$'s last item $IDX_{i}$}
  \BlankLine
  \For{$j\leftarrow IDX_{i} + 1$ \KwTo $|F_1|$}{
    $SET_{ij} = SET_i \cap VDB[j]$ \\
    \If{$|SET_{ij}| \geq Sup_{min}$}{
      $i_{new} = i \cup F_1[j]$ \\
      append $i_{new}$ to $F$ \\
      \EclatRecursive{$i_{new}$, $SET_{ij}$, $j$}
    }
  }
\end{algorithm}

The \emph{EclatRecursive} function will traverse $F_1$ from $i$'s last item.
For each item $F_1[j]$, the function will first intersect its' tid list $VDB[j]$
and $SET_{i}$, the result $SET_{ij}$ represents the transactions that contain both $i$ and $F_1[j]$.
If the size of $SET_{ij}$, which means the new itemset's support, is larger than $Sup_{min}$,
then the union of $i$ and $F_1[j]$, $i_{new}$, will become the new frequent itemset.
Finally the \emph{EclatRecursive} function will be called recursively
to find the new frequent with $i_{new}$ as the prefix. \\

\subsection{Eclat Algorithm vs. Apriori}

The differences between Eclat algorithm and Apriori algorithm are the approach of
searching the frequent itemsets and the data structure of the database. \\
For Apriori algorithm, the frequent itemsets will be searched by breadth-first search (BFS),
which lead to the high-cost computation of counting support. 
By using vertical database and depth-first search (DFS), 
Eclat algorithm can reduce the computation time of counting support 
even the candidate number is larger than Apriori.

\subsubsection{Time Complexity in the Worse Case}\label{time_complexity}

Given a dataset with $ntrans$ transaction, $nitems$ items and average transaction length $tlen$,
we can analyze the complexity of Eclat algorithm and Apriori.
Initially, the time complexity of both algorithms are $O(ntrans \times tlen)$ for building
the itemset list and vertical database. \\
By searching frequent itemsets by Apriori, 
the time complexity is $O(nitems \times ntrans \times tlen)$,
since in the worse case, there will have $nitems$ iterations, and for each iteration,
Apriori will traverse the whole trasaction list ($O(ntrans \times tlen)$) 
to calculate the support for corresponding itemsets. \\
For Eclat algorithm, the time complexity is $O(2^{nitems} - 1) \times O(ntrans)$
searching the frequent itemsets, since in the worst case, the algorithm will perform
intersection operation $2^{nitems} -1$ times, and for each intersection operation,
the time complexity will be $O(ntrans)$ in the worse case. \\

\subsubsection{Experiment}

The computation time of task3 and the comparison with task1 is shown as follows:

\begin{table}[ht]
  \centering
    \begin{tabular}{|*{4}{c|}}
        \hline
    Dataset    & Minimum Support (\%)  & Computation Time (sec) & Speedup Percentage (\%)  \\
        \hline
    \multirow[t]{3}{*}{A}           
                & \multirow[t]{3}{*}{}
                0.2            & 2.92 & 97.96\% \\  \cline{2-4}
                & 0.5          & 0.19 & 97.17\% \\  \cline{2-4}
                & 1.0          & 0.06 & 97.84\% \\  \cline{1-4}         
                B & \multirow[t]{3}{*}{}
                0.15            & 67.09 & 99.02\% \\  \cline{2-4}
                & 0.2          & 40.32 & 98.94\% \\  \cline{2-4}
                & 0.5          & 6.93 & 99.37\% \\  \cline{1-4}
                C & \multirow[t]{3}{*}{}
                1.0            & 65.37 & 98.92\% \\  \cline{2-4}
                & 2.0          & 27.88 & 98.60\% \\  \cline{2-4}
                & 3.0          & 13.61 & 98.13\% \\ 
        \hline
    \end{tabular}
  \caption{Computation Time of Step3}
\end{table}

As we can see above, the computation time of each dataset and minimums support setting
is much faster than Apriori algorithm. By checking the \emph{result2.txt} file,
we can see that the with larger candidate number before pruning,
the speedup percentage will get higher, which means the support counting method of Eclat
is more efficient than Apriori. \\

\subsection{Scalability of Eclat Algorithm}

For testing the scalability of Eclat algorithm, I executing the Eclat program with
fixed minimum support and datset with different $ntrans$, the result is shown as follows:

\newpage

\begin{table}[h]
  \centering
    \begin{tabular}{|*{4}{c|}}
        \hline
    Minimum Support (\%) & Dataset & Computation Time (sec) \\
        \hline
    \multirow[t]{3}{*}{}           
                0.1& \multirow[t]{3}{*}{}
                A            & 1407.57 \\  \cline{2-3}
                & B          & 124.47 \\  \cline{2-3}
                & C          & 2864.68 \\  \cline{1-3}    
                0.15& \multirow[t]{3}{*}{}
                A            &  2.94 \\  \cline{2-3}
                & B          & 67.09 \\  \cline{2-3}
                & C          & 1061.82 \\  \cline{1-3}         
                0.2 & \multirow[t]{3}{*}{}
                A            & 2.92 \\  \cline{2-3}
                & B          & 40.32 \\  \cline{2-3}
                & C          & 656.96 \\  \cline{1-3}
                0.5 & \multirow[t]{3}{*}{}
                A            & 0.19 \\  \cline{2-3}
                & B          & 6.93 \\  \cline{2-3}
                & C          & 116.97 \\  \cline{1-3}
                1.0 & \multirow[t]{3}{*}{}
                A            & 0.06 \\  \cline{2-3}
                & B          & 3.81 \\  \cline{2-3}
                & C          & 65.37 \\  \cline{1-3}
                0.2 & \multirow[t]{3}{*}{}
                A            & 0.02 \\  \cline{2-3}
                & B          & 1.83 \\  \cline{2-3}
                & C          & 27.88 \\  \cline{1-3}
                0.3 & \multirow[t]{3}{*}{}
                A            & 0.01 \\  \cline{2-3}
                & B          & 0.92 \\  \cline{2-3}
                & C          & 13.61 \\ 
        \hline
    \end{tabular}
  \caption{Computation Time of Scalability Test}
\end{table}

It's obviously to observe that the computation time will increase considerablely
when the number of transaction increases. Since the $ntrans$ will affect the computation time
of building the vertical database and the length of each itemset's tid list, which will lead to
longer computation time. \\
The other observation is that with $Sup_{min} = 0.1\%$, the computation time of dataset A
is abnormally large, so the following section will discuss this phenomenon.

\subsection{Disadvantages of Eclat Algorithm}

Although Eclat algorithm is more efficient than Apriori algorithm,
we can still observe the restriction of Eclat algorithm in \ref{time_complexity},
since there might have 


\endgroup

% \begin{figure*}[tbh]
%     \includegraphics[width=\textwidth]{"./student_forum/SAGAN_flow.pdf"}
%     \caption{The architecture of SAGAON.}
%     \label{fig:SAGAON}
% \end{figure*}

% \begin{figure}[tbh]
%     \centering
%     \begin{subfigure}{.5\columnwidth}
%       \centering
%       \includegraphics[width=\linewidth]{"./student_forum/station_level_RMSE.pdf"}
%       \caption{Station-level RMSE}
%     \end{subfigure}%
%     \hfill
%     \begin{subfigure}{.5\columnwidth}
%       \centering
%       \includegraphics[width=\linewidth]{"./student_forum/station_level_MAPE.pdf"}
%       \caption{Station-level MAPE}
%     \end{subfigure}%
%     \caption{SAGAON compares with different baseline models.}
%     \label{fig2}
% \end{figure}

\bibliographystyle{unsrt} % We choose the "plain" reference style
\bibliography{reference} % Entries are in the "references.bib" file

\end{document}